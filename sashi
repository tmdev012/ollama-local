#!/bin/bash
# SASHI - Smart AI Shell Interface
# Routes queries to DeepSeek (fast) or Llama (offline)

set -e
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
source "$SCRIPT_DIR/.env" 2>/dev/null || true

VERSION="1.0.0"
DB_PATH="$SCRIPT_DIR/db/history.db"

# Colors
RED='\033[0;31m'
GREEN='\033[0;32m'
BLUE='\033[0;34m'
YELLOW='\033[1;33m'
NC='\033[0m'

show_help() {
    echo -e "${BLUE}SASHI${NC} - Smart AI Shell Interface v$VERSION"
    echo ""
    echo "Usage: sashi <command> [prompt]"
    echo ""
    echo "Commands:"
    echo "  ask <prompt>      Quick question (DeepSeek)"
    echo "  code <prompt>     Code generation (DeepSeek)"
    echo "  local <prompt>    Offline mode (Llama 3.2)"
    echo "  chat              Interactive chat (DeepSeek)"
    echo "  chat --local      Interactive chat (Llama)"
    echo "  history           Show query history"
    echo "  status            System status"
    echo "  models            List available models"
    echo "  help              Show this help"
    echo ""
    echo "Pipe support:"
    echo "  cat file.py | sashi code 'explain this'"
    echo "  git diff | sashi code 'review this'"
}

log_query() {
    local model="$1"
    local prompt="$2"
    local resp_len="$3"
    local duration="$4"
    python3 -c "
import sqlite3
conn = sqlite3.connect('$DB_PATH')
c = conn.cursor()
c.execute('INSERT INTO queries (model, prompt, response_length, duration_ms) VALUES (?, ?, ?, ?)',
          ('$model', '''$prompt''', $resp_len, $duration))
conn.commit()
" 2>/dev/null || true
}

deepseek_query() {
    local prompt="$1"
    local start_time=$(date +%s%3N)

    local response=$(curl -s https://api.deepseek.com/v1/chat/completions \
      -H "Content-Type: application/json" \
      -H "Authorization: Bearer $DEEPSEEK_API_KEY" \
      -d "{
        \"model\": \"$DEFAULT_MODEL\",
        \"messages\": [{\"role\": \"user\", \"content\": $(echo "$prompt" | jq -Rs .)}],
        \"temperature\": 0.7,
        \"max_tokens\": 2000
      }" | jq -r '.choices[0].message.content // .error.message // "Error: No response"')

    local end_time=$(date +%s%3N)
    local duration=$((end_time - start_time))
    local resp_len=${#response}

    echo "$response"
    log_query "deepseek" "$prompt" "$resp_len" "$duration"
}

llama_query() {
    local prompt="$1"
    local start_time=$(date +%s%3N)

    if ! systemctl is-active ollama &>/dev/null; then
        echo -e "${RED}Ollama not running. Start with: sudo systemctl start ollama${NC}"
        return 1
    fi

    local response=$(ollama run "$LOCAL_MODEL" "$prompt")

    local end_time=$(date +%s%3N)
    local duration=$((end_time - start_time))
    local resp_len=${#response}

    echo "$response"
    log_query "llama3.2" "$prompt" "$resp_len" "$duration"
}

show_status() {
    echo -e "${BLUE}SASHI System Status${NC}"
    echo "===================="

    # Ollama
    if systemctl is-active ollama &>/dev/null; then
        echo -e "Ollama:    ${GREEN}Running${NC}"
    else
        echo -e "Ollama:    ${RED}Stopped${NC}"
    fi

    # DeepSeek
    if [ -n "$DEEPSEEK_API_KEY" ]; then
        echo -e "DeepSeek:  ${GREEN}Configured${NC}"
    else
        echo -e "DeepSeek:  ${RED}No API key${NC}"
    fi

    # Models
    echo ""
    echo "Local Models:"
    ollama list 2>/dev/null || echo "  (none)"

    # Stats
    echo ""
    echo "Query Stats:"
    python3 -c "
import sqlite3
conn = sqlite3.connect('$DB_PATH')
c = conn.cursor()
c.execute('SELECT COUNT(*) FROM queries')
total = c.fetchone()[0]
c.execute('SELECT model, COUNT(*) FROM queries GROUP BY model')
by_model = c.fetchall()
print(f'  Total queries: {total}')
for m, cnt in by_model:
    print(f'  {m}: {cnt}')
" 2>/dev/null || echo "  No history yet"
}

show_history() {
    python3 -c "
import sqlite3
conn = sqlite3.connect('$DB_PATH')
c = conn.cursor()
c.execute('SELECT id, timestamp, model, substr(prompt, 1, 50), duration_ms FROM queries ORDER BY id DESC LIMIT 20')
rows = c.fetchall()
if rows:
    print(f'{'ID':<4} {'Time':<20} {'Model':<10} {'Prompt':<50} {'ms':<6}')
    print('-' * 94)
    for r in rows:
        prompt = r[3].replace('\n', ' ')[:47] + '...' if len(r[3]) > 47 else r[3].replace('\n', ' ')
        print(f'{r[0]:<4} {r[1]:<20} {r[2]:<10} {prompt:<50} {r[4]:<6}')
else:
    print('No history yet')
"
}

interactive_chat() {
    local use_local="$1"

    if [ "$use_local" = "--local" ]; then
        echo -e "${YELLOW}SASHI Chat (Llama 3.2 - Offline)${NC}"
        echo "Type 'exit' to quit"
        echo ""
        ollama run "$LOCAL_MODEL"
    else
        echo -e "${BLUE}SASHI Chat (DeepSeek)${NC}"
        echo "Type 'exit' to quit"
        echo "=============================="

        while true; do
            echo -n -e "${GREEN}> ${NC}"
            read -r prompt

            [ "$prompt" = "exit" ] || [ "$prompt" = "quit" ] && break
            [ -z "$prompt" ] && continue

            echo ""
            deepseek_query "$prompt"
            echo ""
        done
    fi
}

# Read stdin if available
STDIN_DATA=""
if [ ! -t 0 ]; then
    STDIN_DATA=$(cat -)
fi

# Main command routing
case "${1:-help}" in
    ask)
        shift
        prompt="${STDIN_DATA}${STDIN_DATA:+ }$*"
        [ -z "$prompt" ] && { echo "Usage: sashi ask <prompt>"; exit 1; }
        deepseek_query "$prompt"
        ;;
    code)
        shift
        prompt="You are a coding assistant. Be concise. ${STDIN_DATA}${STDIN_DATA:+ }$*"
        [ -z "$*" ] && [ -z "$STDIN_DATA" ] && { echo "Usage: sashi code <prompt>"; exit 1; }
        deepseek_query "$prompt"
        ;;
    local)
        shift
        prompt="${STDIN_DATA}${STDIN_DATA:+ }$*"
        [ -z "$prompt" ] && { echo "Usage: sashi local <prompt>"; exit 1; }
        llama_query "$prompt"
        ;;
    chat)
        interactive_chat "$2"
        ;;
    history)
        show_history
        ;;
    status)
        show_status
        ;;
    models)
        ollama list
        ;;
    help|--help|-h)
        show_help
        ;;
    *)
        # Default: quick ask
        prompt="${STDIN_DATA}${STDIN_DATA:+ }$*"
        [ -z "$prompt" ] && { show_help; exit 1; }
        deepseek_query "$prompt"
        ;;
esac
